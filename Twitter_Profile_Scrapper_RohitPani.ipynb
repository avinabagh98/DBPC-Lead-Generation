{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Profile Scrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use\n",
    "\n",
    "### Setup:\n",
    "--> Ensure you have Python installed.\n",
    "\n",
    "--> Install the required libraries using pip (selenium, pandas, webdriver_manager, etc.).\n",
    "\n",
    "### Run the Script:\n",
    "--> Execute the provided script in your terminal or IDE.\n",
    "\n",
    "### Input Usernames:\n",
    "--> When prompted, enter the number of Twitter profiles you wish to scrape.\n",
    "\n",
    "--> Provide the usernames for the entered number of profiles.\n",
    "\n",
    "### Login to Twitter:\n",
    "--> A browser window (Chrome) will open, taking you to the Twitter login page.\n",
    "\n",
    "--> Manually log in to your Twitter account within the allotted 25 seconds.\n",
    "\n",
    "### Wait:\n",
    "--> The script will automatically visit each profile, scrape the desired information, and store it in memory.\n",
    "\n",
    "### Check the Output:\n",
    "--> Once the script completes, find two Excel files in the script's directory: twitter_data.xlsx (contains profile information) and twitter_hashtags.xlsx (contains hashtags from the tweets).\n",
    "\n",
    "### Done!:\n",
    "--> Review the scraped data in the Excel file.\n",
    "\n",
    "--> If any data is not present, it will leave a blank space in that column.\n",
    "                                                            \n",
    "######                                                           HaPPy ScraPing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Necessary library \n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "\n",
    "# Function to introduce a random delay time\n",
    "def delay():\n",
    "    time.sleep(random.uniform(3, 7))\n",
    "\n",
    "# Function to scrape a given Twitter profile\n",
    "def scrape_twitter_profile(username, num_tweets):\n",
    "    delay()  # Introducing a delay before scraping\n",
    "    URL = \"https://twitter.com/\" + username + \"?lang=en\"\n",
    "    driver.get(URL)\n",
    "    delay()\n",
    "\n",
    "    # Waiting for the tweets to load on the page\n",
    "    try:\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, '[data-testid=\"tweet\"]')))\n",
    "    except WebDriverException:\n",
    "        print(f\"Tweets did not appear for {username}! Proceeding after timeout\")\n",
    "\n",
    "    data = {}\n",
    "    # Extracting profile information with try-except blocks for each data point\n",
    "    try:\n",
    "        data[\"Name\"] = driver.find_element(By.CSS_SELECTOR,'div[data-testid=\"UserName\"]').text.split('\\n')[0]\n",
    "    except:\n",
    "        data[\"Name\"] = None\n",
    "\n",
    "    try:\n",
    "        data[\"Bio\"] = driver.find_element(By.CSS_SELECTOR,'div[data-testid=\"UserDescription\"]').text\n",
    "    except:\n",
    "        data[\"Bio\"] = None\n",
    "\n",
    "    try:\n",
    "        data[\"Location\"] = driver.find_element(By.CSS_SELECTOR,'span[data-testid=\"UserLocation\"]').text\n",
    "    except:\n",
    "        data[\"Location\"] = None\n",
    "\n",
    "    try:\n",
    "        data[\"Website\"] = driver.find_element(By.CSS_SELECTOR,'a[data-testid=\"UserUrl\"]').text\n",
    "    except:\n",
    "        data[\"Website\"] = None\n",
    "\n",
    "    try:\n",
    "        data[\"Joined on\"] = driver.find_element(By.CSS_SELECTOR,'span[data-testid=\"UserJoinDate\"]').text\n",
    "    except:\n",
    "        data[\"Joined on\"] = None\n",
    "\n",
    "    try:\n",
    "        data[\"Following\"] = driver.find_element(By.XPATH, \"//span[contains(text(), 'Following')]/ancestor::a/span\").text\n",
    "    except:\n",
    "        data[\"Following\"] = None\n",
    "\n",
    "    try:\n",
    "        data[\"Followers\"] = driver.find_element(By.XPATH, \"//span[contains(text(), 'Followers')]/ancestor::a/span\").text\n",
    "    except:\n",
    "        data[\"Followers\"] = None\n",
    "\n",
    "    delay()\n",
    "\n",
    "    # Extracting the required number of tweets and their hashtags\n",
    "    tweets = driver.find_elements(By.CSS_SELECTOR, '[data-testid=\"tweet\"]')[:num_tweets]\n",
    "    hashtags = {\"Name\": username}\n",
    "    for i, tweet in enumerate(tweets):\n",
    "        delay()\n",
    "        try:\n",
    "            tweet_text = tweet.find_element(By.CSS_SELECTOR,'div[data-testid=\"tweetText\"]').text\n",
    "            data[f\"Tweet {i+1}\"] = tweet_text\n",
    "            hashtags[f\"Tweet {i+1}\"] = ', '.join(re.findall(r\"(#\\w+)\", tweet_text))\n",
    "        except:\n",
    "            data[f\"Tweet {i+1}\"] = None\n",
    "            hashtags[f\"Tweet {i+1}\"] = None\n",
    "        delay()\n",
    "\n",
    "    return data, hashtags\n",
    "\n",
    "# Getting user input for number of profiles and their usernames\n",
    "num_profiles = int(input(\"Enter the number of Twitter profiles you want to scrape: \"))\n",
    "usernames = [input(f\"Enter username {i+1}: \") for i in range(num_profiles)]\n",
    "num_tweets_to_scrape = 3\n",
    "\n",
    "# Initializing Chrome web driver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Opening Twitter login page and wait for manual user login\n",
    "driver.get(\"https://twitter.com/login?lang=en\")\n",
    "print(\"Please login manually within the next 25 seconds...\")\n",
    "time.sleep(25)\n",
    "\n",
    "# Lists to store scraped data\n",
    "data_list = []\n",
    "hashtags_list = []\n",
    "\n",
    "# Scraping each profile\n",
    "for username in usernames:\n",
    "    data, hashtags = scrape_twitter_profile(username, num_tweets_to_scrape)\n",
    "    data_list.append(data)\n",
    "    hashtags_list.append(hashtags)\n",
    "\n",
    "    print(f\"Scraping completed for {username}\")\n",
    "    delay()\n",
    "\n",
    "print(\"Scraping completed for all profiles. Now processing data...\")\n",
    "\n",
    "# Saving data into Excel files\n",
    "df = pd.DataFrame(data_list)\n",
    "df.to_excel(\"twitter_data.xlsx\", index=False)\n",
    "\n",
    "df_hashtags = pd.DataFrame(hashtags_list)\n",
    "df_hashtags.to_excel(\"twitter_hashtags.xlsx\", index=False)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "print(\"Data processing and saving completed.\")\n",
    "print(\"Thank you!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
